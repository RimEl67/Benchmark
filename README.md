# ğŸš€ Benchmark de performances des Web Services REST  
# par: Rim EL ABBASSI â€” Brahim EL MAJDAOUI


Ce dÃ©pÃ´t prÃ©sente lâ€™Ã©tude comparative de plusieurs implÃ©mentations de services REST.  
Lâ€™objectif est dâ€™analyser leur performance, leur consommation de ressources, ainsi que leur comportement sous diffÃ©rentes charges applicatives.

---

## ğŸ“Œ 1. Objectif du Benchmark

Lâ€™Ã©tude vise Ã  :

- Comparer plusieurs approches pour exposer des API REST (frameworks, variantes Spring, etc.).
- Mesurer leur capacitÃ© Ã  gÃ©rer des scÃ©narios variÃ©s : lecture intensive, jointures, Ã©critures, payloads lourds.
- Ã‰valuer lâ€™impact sur les ressources systÃ¨me (CPU, mÃ©moire, threads, pool de connexions).
- Identifier les points forts, limites et domaines dâ€™utilisation recommandÃ©s pour chaque technologie.

---

## ğŸ§© 2. Architecture et Pile Technologique

Le benchmark repose sur une architecture standard composÃ©e de :

- **Un service Web** implÃ©mentÃ© en plusieurs variantes (par exemple Jersey, Spring MVC, Spring Data RESTâ€¦).
- **Une base de donnÃ©es PostgreSQL**.
- **Un outil de test de charge** pour gÃ©nÃ©rer des scÃ©narios de charge rÃ©alistes.
- **Un systÃ¨me de monitoring** pour lâ€™observation JVM et systÃ¨me.

### Technologies utilisÃ©es

- **Java 21 â€” HotSpot VM**
- **Spring Boot / JAX-RS (selon variante)**
- **PostgreSQL**
- **Docker & Docker Compose**
- **JMeter** pour les tests de charge
- **Prometheus + Grafana** pour le monitoring
- **HikariCP** pour la gestion des connexions SQL

---

## ğŸ“‚ 3. ScÃ©narios de Test

Plusieurs scÃ©narios couvrant les usages les plus courants ont Ã©tÃ© dÃ©finis :

### ğŸ”¹ 3.1. READ-heavy  
Tests de forte sollicitation en lecture, incluant la rÃ©cupÃ©ration dâ€™entitÃ©s avec relations.

### ğŸ”¹ 3.2. JOIN-filter  
ScÃ©narios ciblant la performance des requÃªtes avec filtrage + jointures SQL.

### ğŸ”¹ 3.3. MIXED  
Alternance dâ€™opÃ©rations CREATE, READ, UPDATE, DELETE, reflÃ©tant un usage applicatif rÃ©el.

### ğŸ”¹ 3.4. HEAVY-body  
Tests impliquant des payloads volumineux (JSON large).

---

## ğŸ“Š 4. MÃ©triques ObservÃ©es

Les tests ont permis de collecter plusieurs indicateurs essentiels :

### ğŸ”¸ Performance API
- **DÃ©bit (RPS)** : nombre de requÃªtes traitÃ©es par seconde.
- **Latences** : p50, p95, p99.
- **Taux dâ€™erreurs** : HTTP 4xx / 5xx.

### ğŸ”¸ Ressources JVM
- Utilisation **CPU**
- Consommation **heap**
- Temps **GC**
- **Threads actifs**
- **Connexions Hikari** utilisÃ©es

### ğŸ”¸ Observation systÃ¨me
- I/O disque
- RÃ©seau
- Charge CPU globale

---

## ğŸ§ª 5. MÃ©thodologie

Les tests suivent une mÃ©thodologie reproductible :

1. **DÃ©ploiement** de chaque variante dans un environnement isolÃ© Docker.
2. **PrÃ©paration des donnÃ©es** (dataset initial).
3. **ExÃ©cution automatisÃ©e** des scÃ©narios JMeter.
4. **Instrumentation** via Prometheus.
5. **Analyse** via des dashboards Grafana dÃ©diÃ©s.
6. **Comparaison qualitative et quantitative** des comportements.

---

## ğŸ“Œ 6. CritÃ¨res de Comparaison

Chaque variante a Ã©tÃ© comparÃ©e selon :

- **Performance brute** (dÃ©bit et latence)
- **StabilitÃ© sous charge**
- **SimplicitÃ© de dÃ©veloppement**
- **FacilitÃ© dâ€™exposition des relations**
- **Consommation CPU/RAM**
- **Robustesse face aux payloads lourds**
- **Risque de N+1 / lazy-loading**

---


